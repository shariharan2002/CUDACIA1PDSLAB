{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Op_W4jCUD4F3",
        "outputId": "a722fb62-3bfd-4fad-fe12-140241d1192b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Oct 25 03:55:58 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwkxC-N5Ek9p",
        "outputId": "992ca808-b113-4440-c0f2-2b8dca6d7481"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-a4xm_4ec\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-a4xm_4ec\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 0a71d56e5dce3ff1f0dd2c47c29367629262f527\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4295 sha256=41c85afe4f88d86955e9b979f7c7b107d46a1036118947108cf47b33b4fc1501\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h5i9suyt/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FimVjRP0Eoje",
        "outputId": "b26be408-11e4-4a1c-c637-3ac9c8c9a461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "//Program 1 - Matrix Addition, Not sure if there for CIA-1 LAB or not\n",
        "\n",
        "#include<stdio.h>\n",
        "#include<cuda.h>\n",
        "// Define the no of rows as nx, columns as ny below....\n",
        "#define nx 4\n",
        "#define ny 10\n",
        "\n",
        "__global__ void madd(int *a, int *b, int *c)\n",
        "{\n",
        "    int ix=threadIdx.x + blockIdx.x*blockDim.x;\n",
        "    int iy=threadIdx.y + blockIdx.y*blockDim.y;\n",
        "    int linear_address = iy*nx + ix;\n",
        "    c[linear_address]=a[linear_address]+b[linear_address];\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int *a, *b, *c;\n",
        "    int i;\n",
        "    int ix, iy;\n",
        "    int size = nx * ny * sizeof(int);\n",
        "    a = (int*)malloc(size);\n",
        "    b = (int*)malloc(size);\n",
        "    c = (int*)malloc(size);\n",
        "\n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc((void**)&d_a, size);\n",
        "    cudaMalloc((void**)&d_b, size);\n",
        "    cudaMalloc((void**)&d_c, size);\n",
        "\n",
        "\n",
        "    for(i=0; i<nx*ny; i++)\n",
        "    {\n",
        "        a[i] = i;\n",
        "        b[i] = i+1;\n",
        "    }\n",
        "    printf(\"Matrix A \\n\");\n",
        "\n",
        "    for(ix=0; ix<nx; ix++)\n",
        "    {\n",
        "        for(iy=0; iy<ny; iy++)\n",
        "        {\n",
        "            printf(\"\\t %d\", a[iy+ix*ny]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"Matrix B \\n\");\n",
        "\n",
        "    for(ix=0; ix<nx; ix++)\n",
        "    {\n",
        "        for(iy=0; iy<ny; iy++)\n",
        "        {\n",
        "            printf(\"\\t %d\", b[iy+ix*ny]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    dim3 block(2,2);\n",
        "    dim3 grid((nx+block.x-1)/block.x,(ny+block.y-1)/block.y);\n",
        "\n",
        "    madd<<<grid, block>>>(d_a, d_b, d_c);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaMemcpy(c, d_c, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\n\\n Destination Matrix C \\n\\n\");\n",
        "\n",
        "    for(ix=0; ix<nx; ix++)\n",
        "    {\n",
        "        for(iy=0; iy<ny; iy++)\n",
        "        {\n",
        "            printf(\"\\t %d\", c[iy+ix*ny]);\n",
        "        }\n",
        "        printf(\"\\n\");\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_c);\n",
        "    free(a);\n",
        "    free(b);\n",
        "    free(c);\n",
        "    cudaDeviceReset();\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woL45C-iEpV-",
        "outputId": "b6a76454-c57f-432b-f261-b9a42aa9ff66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix A \n",
            "\t 0\t 1\t 2\t 3\t 4\t 5\t 6\t 7\t 8\t 9\n",
            "\t 10\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\n",
            "\t 20\t 21\t 22\t 23\t 24\t 25\t 26\t 27\t 28\t 29\n",
            "\t 30\t 31\t 32\t 33\t 34\t 35\t 36\t 37\t 38\t 39\n",
            "Matrix B \n",
            "\t 1\t 2\t 3\t 4\t 5\t 6\t 7\t 8\t 9\t 10\n",
            "\t 11\t 12\t 13\t 14\t 15\t 16\t 17\t 18\t 19\t 20\n",
            "\t 21\t 22\t 23\t 24\t 25\t 26\t 27\t 28\t 29\t 30\n",
            "\t 31\t 32\t 33\t 34\t 35\t 36\t 37\t 38\t 39\t 40\n",
            "\n",
            "\n",
            " Destination Matrix C \n",
            "\n",
            "\t 1\t 3\t 5\t 7\t 9\t 11\t 13\t 15\t 17\t 19\n",
            "\t 21\t 23\t 25\t 27\t 29\t 31\t 33\t 35\t 37\t 39\n",
            "\t 41\t 43\t 45\t 47\t 49\t 51\t 53\t 55\t 57\t 59\n",
            "\t 61\t 63\t 65\t 67\t 69\t 71\t 73\t 75\t 77\t 79\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4z-OBVqPFATk",
        "outputId": "0a4b8166-01bc-4c1a-9a59-8b968d341da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Array A \n",
            "0 1 2 3 4 5 6 7 8 9 0 1 \n",
            "2 3 \n",
            "0 1 \n",
            "2 3 \n",
            "1 5 \n",
            "2 8 \n",
            "10 4 \n",
            "18 4 \n",
            "\n",
            "\n",
            " Destination Array A \n",
            "\n",
            "26 1 8 3 4 5 6 7 8 9 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include<stdio.h>\n",
        "#include<cuda.h>\n",
        "#include<math.h>\n",
        "#define N 8\n",
        "// GPT CIA-1 Parallel Reduction Sum of Array\n",
        "__global__ void parallel(int* a)\n",
        "{\n",
        "  int tid=threadIdx.x;\n",
        "  for(int j=0;j<=log2f(N)-1;j++)\n",
        "  {\n",
        "    float z=powf(2,j);\n",
        "    int x=tid%(int)z;\n",
        "    int y=powf(2,j);\n",
        "\n",
        "    if( x==0 && ((2*tid+y) < N) )\n",
        "    {\n",
        "      a[2*tid] = a[2*tid] + a[2*tid + y];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int *a;\n",
        "  int size=N*sizeof(int);\n",
        "  a=(int *)malloc(size);\n",
        "  int *d_a;\n",
        "  cudaMalloc((void**)&d_a,size);\n",
        "  for(int i=0;i<N;i++)\n",
        "  {\n",
        "    a[i]=i+1;\n",
        "  }\n",
        "  printf(\"Elements of A \\n\");\n",
        "  for(int i=0;i<N;i++)\n",
        "  {\n",
        "    printf(\" %d \",a[i]);\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "  cudaMemcpy(d_a,a,size,cudaMemcpyHostToDevice);\n",
        "\n",
        "  parallel<<<(N/2),(N/2)>>>(d_a);\n",
        "\n",
        "  cudaMemcpy(a,d_a,size,cudaMemcpyDeviceToHost);\n",
        "\n",
        "  for(int i=0;i<N;i++)\n",
        "  {\n",
        "    printf(\"%d \",a[i]);\n",
        "  }\n",
        "\n",
        "  cudaFree(d_a);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "4pY3JCqgPM5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c930301c-f106-482c-ba0d-f6bb959d9ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter elements of A: Elements of A \n",
            " 1  2  3  4  5  6  7  8 \n",
            "36 2 7 4 26 6 15 8 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bbg4YxpK8I1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e96d69c3-f3a5-40a3-85d8-b2f4590f9306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Enter N value:Give  Input Nos. : \n",
            "\n",
            "\n",
            " Sorted array:\n",
            "Enter k for finding the kth largest no. :\n",
            "\n",
            "The kth largest no. is : 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rThvgLSdCzwZ",
        "outputId": "32748e85-83fd-4bfd-cff1-e4126e0b8248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 3 4 5 6 7 8 9 10 Thread Indices 6 9 \n",
            "Thread Indices 7 9 \n",
            "Thread Indices 8 9 \n",
            "Thread Indices 9 9 \n",
            "Thread Indices 2 3 \n",
            "Thread Indices 3 3 \n",
            "Thread Indices 4 3 \n",
            "Thread Indices 5 3 \n",
            "Thread Indices 6 3 \n",
            "Thread Indices 7 3 \n",
            "Thread Indices 8 3 \n",
            "Thread Indices 9 3 \n",
            "Thread Indices 0 4 \n",
            "Thread Indices 1 4 \n",
            "Thread Indices 2 4 \n",
            "Thread Indices 3 4 \n",
            "Thread Indices 4 4 \n",
            "Thread Indices 5 4 \n",
            "Thread Indices 6 4 \n",
            "Thread Indices 7 4 \n",
            "Thread Indices 8 4 \n",
            "Thread Indices 9 4 \n",
            "Thread Indices 0 5 \n",
            "Thread Indices 1 5 \n",
            "Thread Indices 2 5 \n",
            "Thread Indices 3 5 \n",
            "Thread Indices 4 5 \n",
            "Thread Indices 5 5 \n",
            "Thread Indices 6 5 \n",
            "Thread Indices 7 5 \n",
            "Thread Indices 8 5 \n",
            "Thread Indices 9 5 \n",
            "Thread Indices 0 6 \n",
            "Thread Indices 1 6 \n",
            "Thread Indices 2 6 \n",
            "Thread Indices 3 6 \n",
            "Thread Indices 4 6 \n",
            "Thread Indices 5 6 \n",
            "Thread Indices 6 6 \n",
            "Thread Indices 7 6 \n",
            "Thread Indices 8 6 \n",
            "Thread Indices 9 6 \n",
            "Thread Indices 0 7 \n",
            "Thread Indices 1 7 \n",
            "Thread Indices 2 7 \n",
            "Thread Indices 3 7 \n",
            "Thread Indices 4 7 \n",
            "Thread Indices 5 7 \n",
            "Thread Indices 6 7 \n",
            "Thread Indices 7 7 \n",
            "Thread Indices 8 7 \n",
            "Thread Indices 9 7 \n",
            "Thread Indices 0 8 \n",
            "Thread Indices 1 8 \n",
            "Thread Indices 2 8 \n",
            "Thread Indices 3 8 \n",
            "Thread Indices 4 8 \n",
            "Thread Indices 5 8 \n",
            "Thread Indices 6 8 \n",
            "Thread Indices 7 8 \n",
            "Thread Indices 8 8 \n",
            "Thread Indices 9 8 \n",
            "Thread Indices 0 9 \n",
            "Thread Indices 1 9 \n",
            "Thread Indices 2 9 \n",
            "Thread Indices 3 9 \n",
            "Thread Indices 4 9 \n",
            "Thread Indices 5 9 \n",
            "Thread Indices 0 0 \n",
            "Thread Indices 1 0 \n",
            "Thread Indices 2 0 \n",
            "Thread Indices 3 0 \n",
            "Thread Indices 4 0 \n",
            "Thread Indices 5 0 \n",
            "Thread Indices 6 0 \n",
            "Thread Indices 7 0 \n",
            "Thread Indices 8 0 \n",
            "Thread Indices 9 0 \n",
            "Thread Indices 0 1 \n",
            "Thread Indices 1 1 \n",
            "Thread Indices 2 1 \n",
            "Thread Indices 3 1 \n",
            "Thread Indices 4 1 \n",
            "Thread Indices 5 1 \n",
            "Thread Indices 6 1 \n",
            "Thread Indices 7 1 \n",
            "Thread Indices 8 1 \n",
            "Thread Indices 9 1 \n",
            "Thread Indices 0 2 \n",
            "Thread Indices 1 2 \n",
            "Thread Indices 2 2 \n",
            "Thread Indices 3 2 \n",
            "Thread Indices 4 2 \n",
            "Thread Indices 5 2 \n",
            "Thread Indices 6 2 \n",
            "Thread Indices 7 2 \n",
            "Thread Indices 8 2 \n",
            "Thread Indices 9 2 \n",
            "Thread Indices 0 3 \n",
            "Thread Indices 1 3 \n",
            "1 1 0 \n",
            "1 2 0 \n",
            "0 3 0 \n",
            "0 4 0 \n",
            "0 5 0 \n",
            "0 6 0 \n",
            "0 7 0 \n",
            "0 8 0 \n",
            "0 9 0 \n",
            "0 10 0 \n",
            "\n",
            "\n",
            " Sorted array:\n",
            "3 1 0 0 0 0 0 0 0 0 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#define N 10\n",
        "// Enumeration Sort\n",
        "__global__ void sort(int *p, int *a, int *s)\n",
        "{\n",
        "    int ix = threadIdx.x;\n",
        "    p[ix] = N-1;\n",
        "\n",
        "    for (int iy = 0; iy < N; iy++)\n",
        "    {\n",
        "        if (a[ix] < a[iy] || (a[ix] == a[iy] && ix < iy))\n",
        "        {\n",
        "            p[ix] -= 1;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    if (ix < N)\n",
        "    {\n",
        "        s[p[ix]] = a[ix];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "    int *p, *a, *s;\n",
        "\n",
        "    int size = N * sizeof(int);\n",
        "    a = (int *)malloc(size);\n",
        "    p = (int *)malloc(size);\n",
        "    s = (int *)malloc(size);\n",
        "\n",
        "    int i;\n",
        "\n",
        "    int *d_p, *d_a, *d_s;\n",
        "    cudaMalloc((void **)&d_p, size);\n",
        "    cudaMalloc((void **)&d_a, size);\n",
        "    cudaMalloc((void **)&d_s, size);\n",
        "\n",
        "    a[0]=100;\n",
        "    a[1]=20;\n",
        "    a[3]=50;\n",
        "    for (i = 4; i < N; i++)\n",
        "    {\n",
        "        a[i] = i+1; // Reverse order for sorting\n",
        "        p[i] = 0;\n",
        "    }\n",
        "\n",
        "    for (i = 0; i < N; i++)\n",
        "    {\n",
        "        printf(\"%d \", a[i]);\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(d_a, a, size, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_p, p, size, cudaMemcpyHostToDevice);\n",
        "\n",
        "    sort<<<1, N>>>(d_p, d_a, d_s);\n",
        "\n",
        "    cudaMemcpy(s, d_s, size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"\\n\\n Sorted array:\\n\");\n",
        "    for (i = 0; i < N; i++)\n",
        "        printf(\"%d \", s[i]);\n",
        "\n",
        "    // Free allocated memory\n",
        "    free(a);\n",
        "    free(p);\n",
        "    free(s);\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_p);\n",
        "    cudaFree(d_s);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Etbz4zAND7_p",
        "outputId": "bff43fc6-6a2b-4a6e-8655-298d912cce84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 20 0 50 5 6 7 8 9 10 \n",
            "\n",
            " Sorted array:\n",
            "0 5 6 7 8 9 10 20 50 100 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "\n",
        "#include<stdio.h>\n",
        "#define N 8\n",
        "// Prefix Sum\n",
        "__global__ void prefixSum(int* a)\n",
        "{\n",
        "  int tid=threadIdx.x;\n",
        "  for(int j=0;j<=log2f(N)-1;j++)\n",
        "  {\n",
        "    float z=powf(2,j);\n",
        "    int x = tid-(int)z;\n",
        "\n",
        "    if(x>=0)\n",
        "    {\n",
        "      a[tid] = a[tid] + a[x];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int *a;\n",
        "  int size=N*sizeof(int);\n",
        "  a=(int *)malloc(size);\n",
        "  int *d_a;\n",
        "  cudaMalloc((void**)&d_a,size);\n",
        "  //printf(\"\\nEnter elements of A: \");\n",
        "  for(int i=0;i<N;i++)\n",
        "  {\n",
        "    a[i]=i+1;\n",
        "  }\n",
        "  cudaMemcpy(d_a,a,size,cudaMemcpyHostToDevice);\n",
        "\n",
        "  prefixSum<<<N,N>>>(d_a);\n",
        "\n",
        "  cudaMemcpy(a,d_a,size,cudaMemcpyDeviceToHost);\n",
        "\n",
        "  for(int i=0;i<N;i++)\n",
        "  {\n",
        "    printf(\"%d \",a[i]);\n",
        "  }\n",
        "\n",
        "  cudaFree(d_a);\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2F33HpdKOl5",
        "outputId": "2a13cedf-8a6d-4269-f45a-56058a3321ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 3 6 10 15 21 28 36 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7vEpxzvLkIJ",
        "outputId": "f3623c69-b16e-428d-db25-43bccea9a812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: 2 6 12 20 30 42 56 72 90 110 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "// GCD\n",
        "__device__ int gcd(int a, int b) {\n",
        "    while (b != 0) {\n",
        "        int temp = b;\n",
        "        b = a % b;\n",
        "        a = temp;\n",
        "    }\n",
        "    return a;\n",
        "}\n",
        "\n",
        "__global__ void gcdKernel(int* vec1, int* vec2, int* result, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < size) {\n",
        "        int a = vec1[idx];\n",
        "        int b = vec2[idx];\n",
        "        result[idx] = gcd(a, b);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 10;\n",
        "    int *h_vec1, *h_vec2;\n",
        "    int *d_vec1, *d_vec2;\n",
        "    int *h_result, *d_result;\n",
        "\n",
        "    h_vec1 = (int*)malloc(n * sizeof(int));\n",
        "    h_vec2 = (int*)malloc(n * sizeof(int));\n",
        "    h_result = (int*)malloc(n * sizeof(int));\n",
        "\n",
        "    cudaMalloc((void**)&d_vec1, n * sizeof(int));\n",
        "    cudaMalloc((void**)&d_vec2, n * sizeof(int));\n",
        "    cudaMalloc((void**)&d_result, n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_vec1[i] = i+4;\n",
        "        h_vec2[i] = i+8;\n",
        "    }\n",
        "\n",
        "    cudaMemcpy(d_vec1, h_vec1, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vec2, h_vec2, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    gcdKernel<<<blocksPerGrid, threadsPerBlock>>>(d_vec1, d_vec2, d_result, n);\n",
        "\n",
        "    cudaMemcpy(h_result, d_result, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"GCD Result: \");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        printf(\"%d \", h_result[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    free(h_vec1);\n",
        "    free(h_vec2);\n",
        "    free(h_result);\n",
        "    cudaFree(d_vec1);\n",
        "    cudaFree(d_vec2);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7Sv9u6uL0X6",
        "outputId": "3c427884-0c08-4df8-f3a5-11aaf8c317f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GCD Result: 4 1 2 1 4 1 2 1 4 1 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "// LCM\n",
        "__device__ int gcd(int a, int b) {\n",
        "    while (b != 0) {\n",
        "        int temp = b;\n",
        "        b = a % b;\n",
        "        a = temp;\n",
        "    }\n",
        "    return a;\n",
        "}\n",
        "\n",
        "__device__ int lcm(int a, int b) {\n",
        "    return (a * b) / gcd(a, b);\n",
        "}\n",
        "\n",
        "__global__ void lcmKernel(int* vec1, int* vec2, int* result, int size) {\n",
        "    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (idx < size) {\n",
        "        int a = vec1[idx];\n",
        "        int b = vec2[idx];\n",
        "        result[idx] = lcm(a, b);\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 10;\n",
        "    int *h_vec1, *h_vec2;\n",
        "    int *d_vec1, *d_vec2;\n",
        "    int *h_result, *d_result;\n",
        "\n",
        "    h_vec1 = (int*)malloc(n * sizeof(int));\n",
        "    h_vec2 = (int*)malloc(n * sizeof(int));\n",
        "    h_result = (int*)malloc(n * sizeof(int));\n",
        "\n",
        "    cudaMalloc((void**)&d_vec1, n * sizeof(int));\n",
        "    cudaMalloc((void**)&d_vec2, n * sizeof(int));\n",
        "    cudaMalloc((void**)&d_result, n * sizeof(int));\n",
        "\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        h_vec1[i] = i + 4;\n",
        "        h_vec2[i] = i + 8;\n",
        "    }\n",
        "\n",
        "\n",
        "    cudaMemcpy(d_vec1, h_vec1, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_vec2, h_vec2, n * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    int threadsPerBlock = 256;\n",
        "    int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;\n",
        "    lcmKernel<<<blocksPerGrid, threadsPerBlock>>>(d_vec1, d_vec2, d_result, n);\n",
        "\n",
        "    cudaMemcpy(h_result, d_result, n * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    printf(\"LCM Result: \");\n",
        "    for (int i = 0; i < n; i++) {\n",
        "        printf(\"%d & %d = %d \\n \", h_vec1[i],h_vec2[i],h_result[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "\n",
        "    free(h_vec1);\n",
        "    free(h_vec2);\n",
        "    free(h_result);\n",
        "    cudaFree(d_vec1);\n",
        "    cudaFree(d_vec2);\n",
        "    cudaFree(d_result);\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Cf5jibcMSgp",
        "outputId": "f6bfd18e-4383-4bf0-b96b-164dcf714ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LCM Result: 4 & 8 = 8 \n",
            " 5 & 9 = 45 \n",
            " 6 & 10 = 30 \n",
            " 7 & 11 = 77 \n",
            " 8 & 12 = 24 \n",
            " 9 & 13 = 117 \n",
            " 10 & 14 = 70 \n",
            " 11 & 15 = 165 \n",
            " 12 & 16 = 48 \n",
            " 13 & 17 = 221 \n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "// Suffix Sum not yet completed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "jtpXTmHAMuV5",
        "outputId": "4e471d19-7cb1-40cb-9e09-cbf0fc74232d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-55-361fcadb86fe>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    // Suffix Sum not yet completed\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQwqCOqSOjhq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}